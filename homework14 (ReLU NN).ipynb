{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milesba4/CS158-ML/blob/main/homework14%20(ReLU%20NN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Homework 14**\n",
        "\n",
        "As usual, we start by recalling many of the functions we've been building all semester:"
      ],
      "metadata": {
        "id": "aqjI5BtDtHoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def TrainTestSplit(X,y,p,seed=1):\n",
        "  '''Splits feature matrix X and target array y into train and test sets\n",
        "  p is the fraction going to train'''\n",
        "  np.random.seed(seed) #controls randomness\n",
        "  size=len(y) \n",
        "  train_size=int(p*size)\n",
        "  train_mask=np.zeros(size,dtype=bool)\n",
        "  train_indices=np.random.choice(size, train_size, replace=False)\n",
        "  train_mask[train_indices]=True \n",
        "  test_mask=~train_mask\n",
        "  X_train=X[train_mask]\n",
        "  X_test=X[test_mask]\n",
        "  y_train=y[train_mask]\n",
        "  y_test=y[test_mask]\n",
        "  return X_train,X_test,y_train,y_test\n",
        "  \n",
        "class Scaler():\n",
        "  def __init__(self,z):\n",
        "    self.min=np.min(z,axis=0)\n",
        "    self.max=np.max(z,axis=0)\n",
        "\n",
        "  def scale(self,x):\n",
        "    return (x-self.min)/(self.max-self.min+0.000001)\n",
        "\n",
        "  def unscale(self,x):\n",
        "    return x*(self.max-self.min+0.000001)+self.min\n",
        "\n",
        "def OneHot(y):\n",
        "  classes=np.max(y)+1\n",
        "  Y=np.zeros((len(y),classes))\n",
        "  i=np.arange(len(y))\n",
        "  Y[i,y[i]]=1\n",
        "  return Y\n",
        "\n",
        "def MSE(pred,y):\n",
        "  return np.mean((pred-y)**2)\n",
        "\n",
        "def Accuracy(pred,y):\n",
        "  '''Assumes pred is an array of probabilities, and y is a one-hot encoded target column'''\n",
        "  class_preds=np.argmax(pred,axis=1) #predicted classes from probabilities\n",
        "  class_target=np.argmax(y,axis=1) #target classes from OneHot encoding\n",
        "  return np.mean(class_preds==class_target)"
      ],
      "metadata": {
        "id": "UCJQdLv2S37L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now recall the Linear, Softmax, and Model classes from the previous assignment:"
      ],
      "metadata": {
        "id": "4zubWFC9tT1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax():\n",
        "  '''Implement Softmax as final layer for prediction only'''\n",
        "  def predict(self,input):\n",
        "    return np.exp(input)/np.sum(np.exp(input),axis=1)[:,np.newaxis]\n",
        "    #the end part [:,np.newaxis] was added just to get the shape right for later use\n",
        "\n",
        "  def backprop(self,grad):\n",
        "    #We ignore this layer in backpropogation\n",
        "    return grad\n",
        "\n",
        "  def update(self,lr):\n",
        "    #Nothing to update\n",
        "    pass"
      ],
      "metadata": {
        "id": "9N0YtR1fsOOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear():\n",
        "  '''Fully connected linear layer class'''\n",
        "  def __init__(self, input_size, output_size):\n",
        "    np.random.seed(input_size) #control randomness! Remove for real use\n",
        "    self.weights = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)\n",
        "    self.biases = np.zeros(output_size)\n",
        "\n",
        "  def predict(self,input):\n",
        "    self.input=input\n",
        "    return self.input@self.weights+self.biases\n",
        "\n",
        "  def backprop(self,grad):\n",
        "    self.grad=grad \n",
        "    return grad@self.weights.T\n",
        "\n",
        "  def update(self,lr):\n",
        "    wt_grad = self.input.T @ self.grad\n",
        "    bias_grad = np.sum(self.grad, axis=0)\n",
        "    self.weights -= lr * wt_grad / len(self.input)\n",
        "    self.biases -= lr * bias_grad / len(self.input)\n"
      ],
      "metadata": {
        "id": "5JxViaK9sVVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "  def __init__(self,layerlist):\n",
        "    self.layerlist=layerlist\n",
        "\n",
        "  def add(self,layer):\n",
        "    self.layerlist+=[layer]\n",
        "\n",
        "  def predict(self,input):\n",
        "    for layer in self.layerlist:\n",
        "      input=layer.predict(input)\n",
        "    return input\n",
        " \n",
        "  def backprop(self,grad):\n",
        "    for layer in self.layerlist[::-1]:\n",
        "      grad=layer.backprop(grad)\n",
        "\n",
        "  def update(self,lr):\n",
        "    for layer in self.layerlist:\n",
        "      layer.update(lr)\n",
        "\n",
        "  def train(self,X,y,epochs,batch_size,lr,loss_fn):\n",
        "    n=len(X)  \n",
        "    indices=np.arange(n)\n",
        "    for i in range(epochs):\n",
        "      np.random.seed(i)\n",
        "      np.random.shuffle(indices)\n",
        "      X_shuffle=X[indices] \n",
        "      y_shuffle=y[indices] \n",
        "      num_batches=n//batch_size\n",
        "      for j in range(num_batches):\n",
        "        X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "        y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "        pred=self.predict(X_batch)\n",
        "        lossgrad=pred-y_batch \n",
        "        #for regression, make sure shape of y_batch is (n,1)\n",
        "        #for Softmax classification, make sure y_batch is OneHot encoded\n",
        "        self.backprop(lossgrad)\n",
        "        self.update(lr)\n",
        "      if n%batch_size!=0: #Check if there is a smaller leftover batch\n",
        "        X_batch=X_shuffle[num_batches*batch_size:] \n",
        "        y_batch=y_shuffle[num_batches*batch_size:] \n",
        "        pred=self.predict(X_batch)\n",
        "        lossgrad=pred-y_batch \n",
        "        self.backprop(lossgrad)\n",
        "        self.update(lr)\n",
        "      if i%100==0: #Change this line to update reporting more/less frequently\n",
        "        print(\"epoch: \",i,\", loss: \",loss_fn(self.predict(X),y))\n"
      ],
      "metadata": {
        "id": "tGGSYCFJu_Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you will write a ReLU layer class. Since ReLU layers have no parameters, there is nothing for the `__init__` or `update` methods to do. Fill in the code for the `predict` and `backprop` methods. "
      ],
      "metadata": {
        "id": "dRThPNxEoRip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU():\n",
        "  '''ReLU layer class'''\n",
        "  def __init__(self):\n",
        "    pass\n",
        "    \n",
        "  def predict(self,input):\n",
        "    self.input=input\n",
        "    return np.maximum(np.zeros(input.shape, dtype = int),input)\n",
        "\n",
        "  def backprop(self,grad):\n",
        "      return grad * (self.input > 0)\n",
        "\n",
        "  def update(self,lr):\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "qencPZyjlytN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test your code on the digits dataset. "
      ],
      "metadata": {
        "id": "6sJDHNpLo6Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "X=load_digits().data\n",
        "y=load_digits().target"
      ],
      "metadata": {
        "id": "d1ulPgG1H25S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, do the following:\n",
        "1. OneHot encode y to obtain Y\n",
        "2. 80/20 Test-train split to obtain Xtrain, Xtest, Ytrain and Ytest\n",
        "3. Scale Xtrain and Xtest to obtain Xtrain_scaled and Xtest_scaled"
      ],
      "metadata": {
        "id": "tCi4VyAnxD8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "Y=OneHot(y)\n",
        "\n",
        "#2\n",
        "Xtrain,Xtest,Ytrain,Ytest = TrainTestSplit(X,Y,.8,seed=1)\n",
        "#Scale training feature matrix and test feature matrix\n",
        "Xtrain_scaler = Scaler(Xtrain)\n",
        "\n",
        "#3\n",
        "Xtrain_scaled = Xtrain_scaler.scale(Xtrain)\n",
        "Xtest_scaled = Xtrain_scaler.scale(Xtest)\n",
        "\n"
      ],
      "metadata": {
        "id": "zHwjJjnbxBi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Neural Network called `digitsNN` with the following:\n",
        "1. A linear layer with 64 inputs and 32 outputs\n",
        "2. A ReLU layer\n",
        "3. A linear layer with 32 inputs and 10 outputs\n",
        "4. A ReLU layer\n",
        "5. A linear layer with 10 inputs and 10 outputs\n",
        "6. A Softmax layer"
      ],
      "metadata": {
        "id": "O-XrMAJAp9xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digitsNN=Model([])\n",
        "digitsNN.add(Linear(64,32))\n",
        "digitsNN.add(ReLU())\n",
        "digitsNN.add(Linear(32,10))\n",
        "digitsNN.add(ReLU())\n",
        "digitsNN.add(Linear(10,10))\n",
        "digitsNN.add(Softmax())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TbpzgjYKqc4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train your Neural Network on Xtrain_scaled and Ytrain using 5000 epochs, batch size of 500, learning rate of 0.01 and Accuracy loss function. "
      ],
      "metadata": {
        "id": "a1YF9T3brEm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digitsNN.train(Xtrain_scaled,Ytrain,5000,500,.01,Accuracy)"
      ],
      "metadata": {
        "id": "RIqVU5ecq7En",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c82e144-46cc-412e-8de6-616506c214d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0 , loss:  0.09672929714683368\n",
            "epoch:  100 , loss:  0.3465553235908142\n",
            "epoch:  200 , loss:  0.6311760612386917\n",
            "epoch:  300 , loss:  0.8093249826026444\n",
            "epoch:  400 , loss:  0.8629088378566457\n",
            "epoch:  500 , loss:  0.8928322894919972\n",
            "epoch:  600 , loss:  0.9109255393180237\n",
            "epoch:  700 , loss:  0.930410577592206\n",
            "epoch:  800 , loss:  0.9394572025052192\n",
            "epoch:  900 , loss:  0.9512874043145442\n",
            "epoch:  1000 , loss:  0.9547668754349339\n",
            "epoch:  1100 , loss:  0.9568545581071677\n",
            "epoch:  1200 , loss:  0.9610299234516354\n",
            "epoch:  1300 , loss:  0.965205288796103\n",
            "epoch:  1400 , loss:  0.9665970772442589\n",
            "epoch:  1500 , loss:  0.9707724425887265\n",
            "epoch:  1600 , loss:  0.9721642310368824\n",
            "epoch:  1700 , loss:  0.9749478079331941\n",
            "epoch:  1800 , loss:  0.9770354906054279\n",
            "epoch:  1900 , loss:  0.9812108559498957\n",
            "epoch:  2000 , loss:  0.9812108559498957\n",
            "epoch:  2100 , loss:  0.9826026443980515\n",
            "epoch:  2200 , loss:  0.9853862212943633\n",
            "epoch:  2300 , loss:  0.9874739039665971\n",
            "epoch:  2400 , loss:  0.9895615866388309\n",
            "epoch:  2500 , loss:  0.9895615866388309\n",
            "epoch:  2600 , loss:  0.9902574808629089\n",
            "epoch:  2700 , loss:  0.9909533750869868\n",
            "epoch:  2800 , loss:  0.9909533750869868\n",
            "epoch:  2900 , loss:  0.9909533750869868\n",
            "epoch:  3000 , loss:  0.9909533750869868\n",
            "epoch:  3100 , loss:  0.9923451635351427\n",
            "epoch:  3200 , loss:  0.9930410577592206\n",
            "epoch:  3300 , loss:  0.9930410577592206\n",
            "epoch:  3400 , loss:  0.9937369519832986\n",
            "epoch:  3500 , loss:  0.9937369519832986\n",
            "epoch:  3600 , loss:  0.9937369519832986\n",
            "epoch:  3700 , loss:  0.9937369519832986\n",
            "epoch:  3800 , loss:  0.9937369519832986\n",
            "epoch:  3900 , loss:  0.9944328462073765\n",
            "epoch:  4000 , loss:  0.9944328462073765\n",
            "epoch:  4100 , loss:  0.9944328462073765\n",
            "epoch:  4200 , loss:  0.9951287404314544\n",
            "epoch:  4300 , loss:  0.9951287404314544\n",
            "epoch:  4400 , loss:  0.9958246346555324\n",
            "epoch:  4500 , loss:  0.9958246346555324\n",
            "epoch:  4600 , loss:  0.9958246346555324\n",
            "epoch:  4700 , loss:  0.9958246346555324\n",
            "epoch:  4800 , loss:  0.9965205288796103\n",
            "epoch:  4900 , loss:  0.9972164231036882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the accuracy of your network on the test sets. It should be a little better than what you got in Homework 11. (Once you get accuracies of over 90%, each percentage point improvement takes a lot of work!)"
      ],
      "metadata": {
        "id": "tEiPI94Wrcgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# acc= digitsNN.train(Xtest_scaled,Ytest,5000,500,.01,Accuracy)\n",
        "acc = Accuracy(digitsNN.predict(Xtest_scaled),Ytest)\n",
        "print(\"actual:\", Y)\n",
        "acc"
      ],
      "metadata": {
        "id": "ZCSpfeUzrh-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78939cc-92b2-4261-f322-9d923c59281c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual: [[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9555555555555556"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}